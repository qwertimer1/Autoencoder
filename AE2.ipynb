{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "# Import comet_ml in the top of your file\n",
    "from comet_ml import Experiment\n",
    "##Needs to be imported before sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn, optim, sigmoid\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import modules\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from torchaudio import transforms\n",
    "#from torchaudio import Datasets\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "#from torchviz import make_dot, make_dot_from_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device type\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_creator(T = 20, L = 10000, N = 1000):\n",
    "    np.random.seed(2)\n",
    "\n",
    "\n",
    "\n",
    "    x = np.empty((N, L), 'int64')\n",
    "    x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "    data = np.sin(x / 1.0 / T).astype('float64')\n",
    "    data = sklearn.preprocessing.normalize(data)\n",
    "\n",
    "    torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to LSTM is sequence length, batch, input_size   \n",
    "#sequence length ---- how many time steps RNN\n",
    "#batch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(10000, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 10000)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sineDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = torch.load('traindata.pt')\n",
    "        if torch.cuda.is_available():\n",
    "            self.data = torch.cuda.FloatTensor(self.data)\n",
    "        else:\n",
    "            self.data = torch.FloatTensor(self.data)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = AE_model().to(device)\n",
    "\n",
    "    \n",
    "    return model, optim.Adam(model.parameters(), lr = lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_dataset, test_dataset, bs):\n",
    "    return (DataLoader(train_dataset, batch_size = batch_size),\n",
    "    DataLoader(test_dataset, batch_size = batch_size * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Need data_creator function\n",
    "\n",
    "data_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 10000\n",
    "latent = 5\n",
    "input_dim = 5\n",
    "epochs = 10000\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "input_size = 997\n",
    "\n",
    "features = 997\n",
    "latent =50\n",
    "\n",
    "\n",
    "lr = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "full_dataset = sineDataset( )\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "train_dl, test_dl = get_data(train_dataset, test_dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_batch, mu, logvar, x):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    MSE = F.binary_cross_entropy(recon_batch, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_function, xb, opt=None):\n",
    "    recon_batch, mu,logvar = model(xb)\n",
    "    loss = loss_function(recon_batch, mu,logvar, xb)\n",
    "   \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for xb in train_dl: \n",
    "            \n",
    "            loss_batch(model, loss_func, xb, opt)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, loss_func, xb)\n",
    "                                    for xb in test_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full training in 3 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model, opt = get_model()\n",
    "#x = torch.randn(1, 10000)\n",
    "#make_dot(model(x), params=dict(model.named_parameters()))\n",
    "\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.7565251517295837\n",
      "0 1.7540676379203797\n",
      "0 1.7520628309249877\n",
      "0 1.7497603797912598\n",
      "0 1.747642960548401\n",
      "0 1.7452585577964783\n",
      "0 1.7432135915756226\n",
      "0 1.740926513671875\n",
      "0 1.7389632487297058\n",
      "0 1.736656813621521\n",
      "0 1.73432599067688\n",
      "0 1.7323632073402404\n",
      "0 1.7301288056373596\n",
      "0 1.7281911849975586\n",
      "0 1.72614422082901\n",
      "0 1.723803119659424\n",
      "0 1.7220211410522461\n",
      "0 1.7199079990386963\n",
      "0 1.7178379726409911\n",
      "0 1.716040141582489\n",
      "0 1.7141786122322082\n",
      "0 1.7119572424888612\n",
      "0 1.709879469871521\n",
      "0 1.7080810809135436\n",
      "0 1.7058292770385741\n",
      "1 1.7042011404037476\n",
      "1 1.7021254777908326\n",
      "1 1.6999658703804017\n",
      "1 1.6982131624221801\n",
      "1 1.69631600856781\n",
      "1 1.6944874668121337\n",
      "1 1.692648687362671\n",
      "1 1.690576777458191\n",
      "1 1.6884821701049804\n",
      "1 1.6868332648277282\n",
      "1 1.6847091937065124\n",
      "1 1.6827119779586792\n",
      "1 1.6813550543785096\n",
      "1 1.679343616962433\n",
      "1 1.6773932576179504\n",
      "1 1.6755475640296935\n",
      "1 1.6737589979171752\n",
      "1 1.6721819591522218\n",
      "1 1.6702309012413026\n",
      "1 1.668628544807434\n",
      "1 1.6668472170829773\n",
      "1 1.6652444553375245\n",
      "1 1.6631862068176269\n",
      "1 1.6618092799186706\n",
      "1 1.659885790348053\n",
      "2 1.658304545879364\n",
      "2 1.656240038871765\n",
      "2 1.6543250942230225\n",
      "2 1.6530128288269044\n",
      "2 1.651241283416748\n",
      "2 1.6494717383384705\n",
      "2 1.6478014254570008\n",
      "2 1.6460941600799561\n",
      "2 1.6443739247322082\n",
      "2 1.6426503539085389\n",
      "2 1.6411439275741577\n",
      "2 1.6395045804977417\n",
      "2 1.6376898717880248\n",
      "2 1.6363447570800782\n",
      "2 1.634389934539795\n",
      "2 1.6329342412948609\n",
      "2 1.631146366596222\n",
      "2 1.6295751452445983\n",
      "2 1.627988224029541\n",
      "2 1.6265959119796753\n",
      "2 1.6250571966171266\n",
      "2 1.6236603450775147\n",
      "2 1.6221043038368226\n",
      "2 1.6206070613861083\n",
      "2 1.6187406158447266\n",
      "3 1.617507905960083\n",
      "3 1.6159155297279357\n",
      "3 1.6141793942451477\n",
      "3 1.6127059745788574\n",
      "3 1.6112317371368408\n",
      "3 1.6094943499565124\n",
      "3 1.6081303715705872\n",
      "3 1.60641850233078\n",
      "3 1.6050313544273376\n",
      "3 1.6031550145149231\n",
      "3 1.6019843244552612\n",
      "3 1.6004380869865418\n",
      "3 1.598933048248291\n",
      "3 1.5974997854232789\n",
      "3 1.5961106848716735\n",
      "3 1.594397919178009\n",
      "3 1.5932519769668578\n",
      "3 1.5917174744606017\n",
      "3 1.5903344893455504\n",
      "3 1.5890435910224914\n",
      "3 1.5876027393341063\n",
      "3 1.5860934782028198\n",
      "3 1.5845541429519654\n",
      "3 1.5831585359573364\n",
      "3 1.5818347096443177\n",
      "4 1.5804679560661317\n",
      "4 1.5791253495216369\n",
      "4 1.5773716950416565\n",
      "4 1.5763240122795106\n",
      "4 1.5747781372070313\n",
      "4 1.5734301233291625\n",
      "4 1.572079110145569\n",
      "4 1.570688281059265\n",
      "4 1.5693784022331239\n",
      "4 1.5679690098762513\n",
      "4 1.566703805923462\n",
      "4 1.5651477193832397\n",
      "4 1.5638177037239074\n",
      "4 1.5623854422569274\n",
      "4 1.5612321138381957\n",
      "4 1.559813928604126\n",
      "4 1.5584400224685668\n",
      "4 1.5571242785453796\n",
      "4 1.5557059717178345\n",
      "4 1.5545636749267577\n",
      "4 1.5533134365081787\n",
      "4 1.5520187902450562\n",
      "4 1.5505807781219483\n",
      "4 1.5493029761314392\n",
      "4 1.548121042251587\n",
      "5 1.5467318463325501\n",
      "5 1.5454736232757569\n",
      "5 1.544196798801422\n",
      "5 1.5428435158729554\n",
      "5 1.5415067791938781\n",
      "5 1.5404181909561157\n",
      "5 1.539042022228241\n",
      "5 1.5376486706733703\n",
      "5 1.536443281173706\n",
      "5 1.5350233149528503\n",
      "5 1.5336854267120361\n",
      "5 1.5323605394363404\n",
      "5 1.5310759568214416\n",
      "5 1.5300058436393738\n",
      "5 1.5286737322807311\n",
      "5 1.5273593854904175\n",
      "5 1.5260459113121032\n",
      "5 1.524844663143158\n",
      "5 1.523657932281494\n",
      "5 1.5225277161598205\n",
      "5 1.5209817433357238\n",
      "5 1.519798574447632\n",
      "5 1.5186005401611329\n",
      "5 1.5172213768959046\n",
      "5 1.5158752942085265\n",
      "6 1.51469979763031\n",
      "6 1.5133694696426392\n",
      "6 1.512148289680481\n",
      "6 1.5107470750808716\n",
      "6 1.5094668412208556\n",
      "6 1.5081810665130615\n",
      "6 1.5069483041763305\n",
      "6 1.505588059425354\n",
      "6 1.504343295097351\n",
      "6 1.5030840849876403\n",
      "6 1.5018423223495483\n",
      "6 1.5005150175094604\n",
      "6 1.4993029642105102\n",
      "6 1.4980723452568054\n",
      "6 1.496778392791748\n",
      "6 1.495501194000244\n",
      "6 1.49416405916214\n",
      "6 1.4931080722808838\n",
      "6 1.4918614959716796\n",
      "6 1.4904947876930237\n",
      "6 1.4892853236198424\n",
      "6 1.4880468249320984\n",
      "6 1.4867767548561097\n",
      "6 1.4853678274154662\n",
      "6 1.4841607809066772\n",
      "7 1.4829516744613647\n",
      "7 1.4816307187080384\n",
      "7 1.4805501651763917\n",
      "7 1.479334888458252\n",
      "7 1.477843894958496\n",
      "7 1.4767793321609497\n",
      "7 1.4755014061927796\n",
      "7 1.4741938495635987\n",
      "7 1.4728932976722717\n",
      "7 1.471532835960388\n",
      "7 1.4703517627716065\n",
      "7 1.4690362215042114\n",
      "7 1.46765043258667\n",
      "7 1.4664944362640382\n",
      "7 1.465277442932129\n",
      "7 1.464165163040161\n",
      "7 1.462929639816284\n",
      "7 1.461581289768219\n",
      "7 1.4604877018928528\n",
      "7 1.4593048667907715\n",
      "7 1.4578752541542053\n",
      "7 1.4568010687828064\n",
      "7 1.4553757619857788\n",
      "7 1.4541106271743773\n",
      "7 1.4529293537139893\n",
      "8 1.451825375556946\n",
      "8 1.4505765104293824\n",
      "8 1.4491155052185059\n",
      "8 1.4479272365570068\n",
      "8 1.4468162703514098\n",
      "8 1.4454907155036927\n",
      "8 1.4442646312713623\n",
      "8 1.44302832365036\n",
      "8 1.4417889952659606\n",
      "8 1.4405069875717162\n",
      "8 1.4392530846595764\n",
      "8 1.4381067609786988\n",
      "8 1.4366589856147767\n",
      "8 1.4355444169044496\n",
      "8 1.434486129283905\n",
      "8 1.433258078098297\n",
      "8 1.4319380497932435\n",
      "8 1.430805332660675\n",
      "8 1.4295519161224366\n",
      "8 1.428541157245636\n",
      "8 1.4272601985931397\n",
      "8 1.4261168718338013\n",
      "8 1.4247170972824097\n",
      "8 1.4236852645874023\n",
      "8 1.4225571250915527\n",
      "9 1.421221878528595\n",
      "9 1.419999418258667\n",
      "9 1.4187951707839965\n",
      "9 1.417708089351654\n",
      "9 1.4165684056282044\n",
      "9 1.4153712582588196\n",
      "9 1.413997757434845\n",
      "9 1.4127954864501953\n",
      "9 1.4116559386253358\n",
      "9 1.4105024886131288\n",
      "9 1.4092505812644958\n",
      "9 1.408121988773346\n",
      "9 1.4068340706825255\n",
      "9 1.4055459642410277\n",
      "9 1.4043794965744019\n",
      "9 1.4032871890068055\n",
      "9 1.4021788763999938\n",
      "9 1.4009312534332274\n",
      "9 1.3998605346679687\n",
      "9 1.3986298608779908\n",
      "9 1.3975958728790283\n",
      "9 1.3964197540283203\n",
      "9 1.3951043796539306\n",
      "9 1.3941206741333008\n",
      "9 1.3928633046150207\n",
      "10 1.3917024660110473\n",
      "10 1.390542404651642\n",
      "10 1.389348783493042\n",
      "10 1.3881178855895997\n",
      "10 1.3870258855819702\n",
      "10 1.3858685064315797\n",
      "10 1.3845863699913026\n",
      "10 1.3835650777816773\n",
      "10 1.3822984290122986\n",
      "10 1.3812112426757812\n",
      "10 1.3800899577140808\n",
      "10 1.3788230323791504\n",
      "10 1.3776455044746398\n",
      "10 1.3764951753616332\n",
      "10 1.3751893210411072\n",
      "10 1.374222936630249\n",
      "10 1.3731584072113037\n",
      "10 1.3719803762435914\n",
      "10 1.3709038448333741\n",
      "10 1.369786069393158\n",
      "10 1.3685058498382567\n",
      "10 1.367500958442688\n",
      "10 1.366387846469879\n",
      "10 1.3651434993743896\n",
      "10 1.364232635498047\n",
      "11 1.3629629230499267\n",
      "11 1.3618443775177003\n",
      "11 1.3607139468193055\n",
      "11 1.3595551300048827\n",
      "11 1.3585340404510498\n",
      "11 1.3571937704086303\n",
      "11 1.3560946154594422\n",
      "11 1.3549568772315979\n",
      "11 1.353837823867798\n",
      "11 1.3527052593231201\n",
      "11 1.3516680669784547\n",
      "11 1.3503773427009582\n",
      "11 1.3492502641677857\n",
      "11 1.3480552625656128\n",
      "11 1.3470543694496155\n",
      "11 1.3458669686317444\n",
      "11 1.3448140001296998\n",
      "11 1.3436647629737855\n",
      "11 1.3424438333511353\n",
      "11 1.3414399743080139\n",
      "11 1.3403740525245667\n",
      "11 1.3392932772636414\n",
      "11 1.3380891919136046\n",
      "11 1.3371553349494933\n",
      "11 1.3358600068092346\n",
      "12 1.3349841237068176\n",
      "12 1.333686113357544\n",
      "12 1.33263694524765\n",
      "12 1.331587872505188\n",
      "12 1.3303832602500916\n",
      "12 1.3292657899856568\n",
      "12 1.3282159924507142\n",
      "12 1.3269990825653075\n",
      "12 1.3258781504631043\n",
      "12 1.3248205137252809\n",
      "12 1.3238398623466492\n",
      "12 1.3226433157920838\n",
      "12 1.3214245343208313\n",
      "12 1.3203203248977662\n",
      "12 1.3192644286155701\n",
      "12 1.3182325792312621\n",
      "12 1.3172039461135865\n",
      "12 1.3161038208007811\n",
      "12 1.3150382947921753\n",
      "12 1.314013888835907\n",
      "12 1.3129324436187744\n",
      "12 1.3117439198493956\n",
      "12 1.310624508857727\n",
      "12 1.3096645188331604\n",
      "12 1.3085580205917358\n",
      "13 1.3074554848670958\n",
      "13 1.3064302921295166\n",
      "13 1.3051567888259887\n",
      "13 1.3043050837516785\n",
      "13 1.30309006690979\n",
      "13 1.3020231986045838\n",
      "13 1.3009591436386108\n",
      "13 1.299873661994934\n",
      "13 1.2987206292152405\n",
      "13 1.2976642370223999\n",
      "13 1.2965478324890136\n",
      "13 1.2955769538879394\n",
      "13 1.294547905921936\n",
      "13 1.2934800505638122\n",
      "13 1.2924474215507507\n",
      "13 1.2913306164741516\n",
      "13 1.290255331993103\n",
      "13 1.2891297006607056\n",
      "13 1.2883171343803406\n",
      "13 1.2870722436904907\n",
      "13 1.2861880469322204\n",
      "13 1.2850958967208863\n",
      "13 1.2838619112968446\n",
      "13 1.282952151298523\n",
      "13 1.2818563675880432\n",
      "14 1.2807797956466676\n",
      "14 1.279675178527832\n",
      "14 1.2788015747070312\n",
      "14 1.277638964653015\n",
      "14 1.276706748008728\n",
      "14 1.2756534957885741\n",
      "14 1.2743624019622803\n",
      "14 1.2734951329231263\n",
      "14 1.2724082112312316\n",
      "14 1.2713478922843933\n",
      "14 1.2703868675231933\n",
      "14 1.2691726541519166\n",
      "14 1.2681393098831177\n",
      "14 1.2670870399475098\n",
      "14 1.2661080718040467\n",
      "14 1.2653081011772156\n",
      "14 1.2641112446784972\n",
      "14 1.26301353931427\n",
      "14 1.2622393369674683\n",
      "14 1.2610354471206664\n",
      "14 1.2600979375839234\n",
      "14 1.2589362454414368\n",
      "14 1.2579372572898864\n",
      "14 1.257037410736084\n",
      "14 1.2559675836563111\n",
      "15 1.2549376392364502\n",
      "15 1.2538480067253113\n",
      "15 1.2530217719078065\n",
      "15 1.251979022026062\n",
      "15 1.2509500503540039\n",
      "15 1.249876115322113\n",
      "15 1.2488694667816163\n",
      "15 1.2478464126586915\n",
      "15 1.2469134640693664\n",
      "15 1.2459499049186706\n",
      "15 1.2447761011123657\n",
      "15 1.2438089680671691\n",
      "15 1.2428586959838868\n",
      "15 1.24171950340271\n",
      "15 1.2407936120033265\n",
      "15 1.239850549697876\n",
      "15 1.2388566684722901\n",
      "15 1.2377989053726197\n",
      "15 1.2368981480598449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 1.2358500242233277\n",
      "15 1.2348474144935608\n",
      "15 1.233779718875885\n",
      "15 1.2326733541488648\n",
      "15 1.2318372440338134\n",
      "15 1.2310085201263428\n",
      "16 1.2299167394638062\n",
      "16 1.2288084483146668\n",
      "16 1.2279849314689637\n",
      "16 1.227078914642334\n",
      "16 1.2260050010681152\n",
      "16 1.2248658061027526\n",
      "16 1.22405503988266\n",
      "16 1.223116774559021\n",
      "16 1.2219315242767335\n",
      "16 1.2209502124786378\n",
      "16 1.2200185441970826\n",
      "16 1.2190482878684998\n",
      "16 1.2180819702148438\n",
      "16 1.217142140865326\n",
      "16 1.2162413001060486\n",
      "16 1.2152642297744751\n",
      "16 1.2139648127555847\n",
      "16 1.2134068655967711\n",
      "16 1.2123581385612487\n",
      "16 1.2114778327941895\n",
      "16 1.2104718637466432\n",
      "16 1.2093919348716735\n",
      "16 1.2085028719902038\n",
      "16 1.2075129413604737\n",
      "16 1.206541223526001\n",
      "17 1.205751633644104\n",
      "17 1.2046029901504516\n",
      "17 1.2037123441696167\n",
      "17 1.202728214263916\n",
      "17 1.2018828916549682\n",
      "17 1.2008601140975952\n",
      "17 1.1999443125724794\n",
      "17 1.1988866496086121\n",
      "17 1.1979435634613038\n",
      "17 1.1971721100807189\n",
      "17 1.1961089777946472\n",
      "17 1.1951845979690552\n",
      "17 1.1939859437942504\n",
      "17 1.1934340286254883\n",
      "17 1.192222466468811\n",
      "17 1.1912373280525208\n",
      "17 1.1903842878341675\n",
      "17 1.1895469236373901\n",
      "17 1.1886101317405702\n",
      "17 1.187994532585144\n",
      "17 1.1866747999191285\n",
      "17 1.1855479073524475\n",
      "17 1.184739294052124\n",
      "17 1.1839168691635131\n",
      "17 1.1828820848464965\n",
      "18 1.182156023979187\n",
      "18 1.1810565209388733\n",
      "18 1.180083565711975\n",
      "18 1.1793636798858642\n",
      "18 1.1783312702178954\n",
      "18 1.1775961899757386\n",
      "18 1.176516706943512\n",
      "18 1.1757501482963562\n",
      "18 1.1748838567733764\n",
      "18 1.1738894009590148\n",
      "18 1.172720501422882\n",
      "18 1.1719304275512696\n",
      "18 1.1709239435195924\n",
      "18 1.1701614022254945\n",
      "18 1.1691290402412415\n",
      "18 1.1683005332946776\n",
      "18 1.1675981616973876\n",
      "18 1.1663795375823975\n",
      "18 1.1656974935531617\n",
      "18 1.1646332573890685\n",
      "18 1.1637928247451783\n",
      "18 1.1630660700798034\n",
      "18 1.161930935382843\n",
      "18 1.1610563588142395\n",
      "18 1.160515651702881\n",
      "19 1.1592746424674987\n",
      "19 1.1585004210472107\n",
      "19 1.1574719524383545\n",
      "19 1.156479766368866\n",
      "19 1.1559000062942504\n",
      "19 1.1550231313705444\n",
      "19 1.1539934539794923\n",
      "19 1.1527574205398559\n",
      "19 1.1521104073524475\n",
      "19 1.1512437748908997\n",
      "19 1.150468466281891\n",
      "19 1.1496803045272828\n",
      "19 1.148811285495758\n",
      "19 1.1476994395256042\n",
      "19 1.1468014430999756\n",
      "19 1.1457726287841796\n",
      "19 1.1450363969802857\n",
      "19 1.1442386078834534\n",
      "19 1.1432613611221314\n",
      "19 1.1427494120597839\n",
      "19 1.1414597511291504\n",
      "19 1.1407446146011353\n",
      "19 1.1399283123016357\n",
      "19 1.1389375591278077\n",
      "19 1.1382474422454834\n",
      "20 1.1376204657554627\n",
      "20 1.1364874196052552\n",
      "20 1.135644235610962\n",
      "20 1.1348691987991333\n",
      "20 1.1338651752471924\n",
      "20 1.1332150530815124\n",
      "20 1.1322487425804137\n",
      "20 1.1314618158340455\n",
      "20 1.1302471137046814\n",
      "20 1.1297314310073852\n",
      "20 1.1289602184295655\n",
      "20 1.1279540824890137\n",
      "20 1.1268889021873474\n",
      "20 1.1261305832862853\n",
      "20 1.1253370833396912\n",
      "20 1.124318242073059\n",
      "20 1.123612356185913\n",
      "20 1.122920467853546\n",
      "20 1.1220473742485046\n",
      "20 1.12087651014328\n",
      "20 1.1200800395011903\n",
      "20 1.1194340515136718\n",
      "20 1.1187141227722168\n",
      "20 1.1177590131759643\n",
      "20 1.1168199515342712\n",
      "21 1.1161257219314575\n",
      "21 1.1152535843849183\n",
      "21 1.114621856212616\n",
      "21 1.11333801984787\n",
      "21 1.1125607895851135\n",
      "21 1.1117287921905517\n",
      "21 1.1107918906211853\n",
      "21 1.1104096674919128\n",
      "21 1.1095666599273681\n",
      "21 1.1085513854026794\n",
      "21 1.1077192759513854\n",
      "21 1.1069898557662965\n",
      "21 1.105943329334259\n",
      "21 1.1053162932395935\n",
      "21 1.104455394744873\n",
      "21 1.1036062479019164\n",
      "21 1.1030727338790893\n",
      "21 1.1020880579948424\n",
      "21 1.1011543011665343\n",
      "21 1.1003227734565735\n",
      "21 1.0995641350746155\n",
      "21 1.0987518453598022\n",
      "21 1.0978357577323914\n",
      "21 1.0969472217559815\n",
      "21 1.0963721942901612\n",
      "22 1.0957355451583863\n",
      "22 1.0947671985626222\n",
      "22 1.0939094233512878\n",
      "22 1.0934973669052124\n",
      "22 1.0923808360099792\n",
      "22 1.0916154170036316\n",
      "22 1.090587089061737\n",
      "22 1.0900864505767822\n",
      "22 1.0890339612960815\n",
      "22 1.0885043716430665\n",
      "22 1.087360553741455\n",
      "22 1.0865944862365722\n",
      "22 1.0858561968803406\n",
      "22 1.0851665472984313\n",
      "22 1.0839079856872558\n",
      "22 1.0832320713996888\n",
      "22 1.0827289724349975\n",
      "22 1.0821634531021118\n",
      "22 1.081263792514801\n",
      "22 1.0805383729934692\n",
      "22 1.0797971105575561\n",
      "22 1.0788382124900817\n",
      "22 1.0780871987342835\n",
      "22 1.077433831691742\n",
      "22 1.0766747307777405\n",
      "23 1.0756429433822632\n",
      "23 1.074928379058838\n",
      "23 1.0739357900619506\n",
      "23 1.0735366201400758\n",
      "23 1.0727321457862855\n",
      "23 1.0718595838546754\n",
      "23 1.0711893796920777\n",
      "23 1.0703541326522827\n",
      "23 1.069518904685974\n",
      "23 1.0686693477630616\n",
      "23 1.0679690504074097\n",
      "23 1.0672540283203125\n",
      "23 1.0668235230445862\n",
      "23 1.0653652453422546\n",
      "23 1.065190966129303\n",
      "23 1.0644094491004943\n",
      "23 1.0634940433502198\n",
      "23 1.0626319456100464\n",
      "23 1.0615038180351257\n",
      "23 1.0610754323005676\n",
      "23 1.0602083420753479\n",
      "23 1.0597537302970885\n",
      "23 1.0590684461593627\n",
      "23 1.0583774518966675\n",
      "23 1.0571770977973938\n",
      "24 1.0567613697052003\n",
      "24 1.055931944847107\n",
      "24 1.0550374913215637\n",
      "24 1.0543747687339782\n",
      "24 1.0535959672927857\n",
      "24 1.0530324506759643\n",
      "24 1.052300088405609\n",
      "24 1.0516294264793395\n",
      "24 1.0503813004493714\n",
      "24 1.0496928882598877\n",
      "24 1.0493463373184204\n",
      "24 1.0484584164619446\n",
      "24 1.047440354824066\n",
      "24 1.0470465350151061\n",
      "24 1.045850625038147\n",
      "24 1.045460045337677\n",
      "24 1.0445880055427552\n",
      "24 1.043969807624817\n",
      "24 1.0432244157791137\n",
      "24 1.0424855089187621\n",
      "24 1.041582272052765\n",
      "24 1.0405649995803834\n",
      "24 1.040532829761505\n",
      "24 1.0395977520942687\n",
      "24 1.0391066908836364\n",
      "25 1.0381807637214662\n",
      "25 1.037208309173584\n",
      "25 1.0364625287055969\n",
      "25 1.0360193419456483\n",
      "25 1.0351911067962647\n",
      "25 1.0347583389282227\n",
      "25 1.0337308502197267\n",
      "25 1.0331172299385072\n",
      "25 1.032456398010254\n",
      "25 1.0315981388092041\n",
      "25 1.031125135421753\n",
      "25 1.0299705171585083\n",
      "25 1.0292757415771485\n",
      "25 1.0287198090553284\n",
      "25 1.0278962826728821\n",
      "25 1.0272026801109313\n",
      "25 1.0268742442131042\n",
      "25 1.0255494809150696\n",
      "25 1.0252928042411804\n",
      "25 1.0248322892189026\n",
      "25 1.0237949562072755\n",
      "25 1.023179018497467\n",
      "25 1.0221259880065918\n",
      "25 1.021905438899994\n",
      "25 1.0209552574157714\n",
      "26 1.020059461593628\n",
      "26 1.0199446773529053\n",
      "26 1.0190523409843444\n",
      "26 1.0183057403564453\n",
      "26 1.017311544418335\n",
      "26 1.0169670963287354\n",
      "26 1.0161217474937438\n",
      "26 1.0155090618133544\n",
      "26 1.014509561061859\n",
      "26 1.0141069173812867\n",
      "26 1.0132804584503174\n",
      "26 1.0128493666648866\n",
      "26 1.0120827174186706\n",
      "26 1.0113496565818787\n",
      "26 1.010419659614563\n",
      "26 1.0099537563323975\n",
      "26 1.0090169763565064\n",
      "26 1.0084874320030213\n",
      "26 1.007623770236969\n",
      "26 1.007007930278778\n",
      "26 1.0063597774505615\n",
      "26 1.0055493474006654\n",
      "26 1.0054980945587157\n",
      "26 1.0042471170425415\n",
      "26 1.0035593581199647\n",
      "27 1.0028056168556214\n",
      "27 1.001920108795166\n",
      "27 1.0021372509002686\n",
      "27 1.0006361961364747\n",
      "27 1.000297884941101\n",
      "27 0.9995348763465881\n",
      "27 0.9989677262306214\n",
      "27 0.9980609297752381\n",
      "27 0.9976964545249939\n",
      "27 0.9968021416664123\n",
      "27 0.9958863472938537\n",
      "27 0.9957692313194275\n",
      "27 0.9946219897270203\n",
      "27 0.9943414235115051\n",
      "27 0.9938998889923095\n",
      "27 0.99323575258255\n",
      "27 0.9922726225852966\n",
      "27 0.9913624835014343\n",
      "27 0.9906582140922546\n",
      "27 0.9905592012405395\n",
      "27 0.9894398832321167\n",
      "27 0.9890502071380616\n",
      "27 0.9885830879211426\n",
      "27 0.9878512859344483\n",
      "27 0.9868242192268372\n",
      "28 0.9868043231964111\n",
      "28 0.9854697966575623\n",
      "28 0.9848845386505127\n",
      "28 0.9845997619628907\n",
      "28 0.9837976098060608\n",
      "28 0.9830057048797607\n",
      "28 0.9824562382698059\n",
      "28 0.9817896223068238\n",
      "28 0.9810363101959229\n",
      "28 0.980607943534851\n",
      "28 0.9798800754547119\n",
      "28 0.9790424823760986\n",
      "28 0.9783107089996338\n",
      "28 0.9777995920181275\n",
      "28 0.9770079922676086\n",
      "28 0.9766289663314819\n",
      "28 0.9760623693466186\n",
      "28 0.9755432367324829\n",
      "28 0.9748009371757508\n",
      "28 0.9740532517433167\n",
      "28 0.973618438243866\n",
      "28 0.9727204751968384\n",
      "28 0.9720024538040161\n",
      "28 0.9717679762840271\n",
      "28 0.9710506200790405\n",
      "29 0.9699358177185059\n",
      "29 0.9693555188179016\n",
      "29 0.969247510433197\n",
      "29 0.9686705398559571\n",
      "29 0.9680114030838013\n",
      "29 0.9668858623504639\n",
      "29 0.9667293000221252\n",
      "29 0.9661905860900879\n",
      "29 0.96525545835495\n",
      "29 0.9645864009857178\n",
      "29 0.9636143207550049\n",
      "29 0.963040542602539\n",
      "29 0.9631467390060425\n",
      "29 0.9620222759246826\n",
      "29 0.9612742304801941\n",
      "29 0.9608534431457519\n",
      "29 0.9602032732963562\n",
      "29 0.9596542501449585\n",
      "29 0.9593503570556641\n",
      "29 0.9582051753997802\n"
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_function, opt, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Page Break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent):\n",
    "        super(AE, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent = latent\n",
    "        self.gru = nn.GRU(input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(input_size, self.latent)\n",
    "        self.gru2 = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        output, hidden = self.gru(input, self.hidden_size)\n",
    "        fc1 = self.linear(output)\n",
    "        output2, hidden = self.gru(fc1, self.hidden_size)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    #KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE #+ KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get x values of the sine wave\n",
    "\n",
    "time        = np.arange(0, 1000, 0.1);\n",
    "\n",
    "# Amplitude of the sine wave is sine of a variable like time\n",
    "\n",
    "amplitude   = np.sin(time)\n",
    "#Create multiple amplitude arrays of varying size\n",
    "amplitude_2 = amplitude * 2\n",
    "amplitude_4 = amplitude *4\n",
    "amplitude_03 = amplitude * 1/3\n",
    "amplitude_02 = amplitude * 1/2\n",
    "amplitude_8 = amplitude * 8\n",
    "amplitudes = {'amplitude_2':amplitude_2, \n",
    "              'amplitude_4':amplitude_4, \n",
    "              'amplitude_8':amplitude_8, \n",
    "              'amplitude_02':amplitude_02,\n",
    "              'amplitude_03':amplitude_03}\n",
    "\n",
    "amplitudes_nd = [amplitude_2, \n",
    "              amplitude_4, \n",
    "              amplitude_8, \n",
    "              amplitude_02,\n",
    "              amplitude_03]\n",
    "\n",
    "\n",
    "eps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(amplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_2</th>\n",
       "      <th>amplitude_4</th>\n",
       "      <th>amplitude_8</th>\n",
       "      <th>amplitude_02</th>\n",
       "      <th>amplitude_03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.199667</td>\n",
       "      <td>0.399334</td>\n",
       "      <td>0.798667</td>\n",
       "      <td>0.049917</td>\n",
       "      <td>0.033278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.397339</td>\n",
       "      <td>0.794677</td>\n",
       "      <td>1.589355</td>\n",
       "      <td>0.099335</td>\n",
       "      <td>0.066223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.591040</td>\n",
       "      <td>1.182081</td>\n",
       "      <td>2.364162</td>\n",
       "      <td>0.147760</td>\n",
       "      <td>0.098507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.778837</td>\n",
       "      <td>1.557673</td>\n",
       "      <td>3.115347</td>\n",
       "      <td>0.194709</td>\n",
       "      <td>0.129806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.958851</td>\n",
       "      <td>1.917702</td>\n",
       "      <td>3.835404</td>\n",
       "      <td>0.239713</td>\n",
       "      <td>0.159809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.129285</td>\n",
       "      <td>2.258570</td>\n",
       "      <td>4.517140</td>\n",
       "      <td>0.282321</td>\n",
       "      <td>0.188214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.288435</td>\n",
       "      <td>2.576871</td>\n",
       "      <td>5.153741</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.214739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.434712</td>\n",
       "      <td>2.869424</td>\n",
       "      <td>5.738849</td>\n",
       "      <td>0.358678</td>\n",
       "      <td>0.239119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.566654</td>\n",
       "      <td>3.133308</td>\n",
       "      <td>6.266615</td>\n",
       "      <td>0.391663</td>\n",
       "      <td>0.261109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.682942</td>\n",
       "      <td>3.365884</td>\n",
       "      <td>6.731768</td>\n",
       "      <td>0.420735</td>\n",
       "      <td>0.280490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.782415</td>\n",
       "      <td>3.564829</td>\n",
       "      <td>7.129659</td>\n",
       "      <td>0.445604</td>\n",
       "      <td>0.297069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.864078</td>\n",
       "      <td>3.728156</td>\n",
       "      <td>7.456313</td>\n",
       "      <td>0.466020</td>\n",
       "      <td>0.310680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.927116</td>\n",
       "      <td>3.854233</td>\n",
       "      <td>7.708465</td>\n",
       "      <td>0.481779</td>\n",
       "      <td>0.321186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.970899</td>\n",
       "      <td>3.941799</td>\n",
       "      <td>7.883598</td>\n",
       "      <td>0.492725</td>\n",
       "      <td>0.328483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.994990</td>\n",
       "      <td>3.989980</td>\n",
       "      <td>7.979960</td>\n",
       "      <td>0.498747</td>\n",
       "      <td>0.332498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.999147</td>\n",
       "      <td>3.998294</td>\n",
       "      <td>7.996589</td>\n",
       "      <td>0.499787</td>\n",
       "      <td>0.333191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.983330</td>\n",
       "      <td>3.966659</td>\n",
       "      <td>7.933318</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0.330555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.947695</td>\n",
       "      <td>3.895391</td>\n",
       "      <td>7.790781</td>\n",
       "      <td>0.486924</td>\n",
       "      <td>0.324616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.892600</td>\n",
       "      <td>3.785200</td>\n",
       "      <td>7.570401</td>\n",
       "      <td>0.473150</td>\n",
       "      <td>0.315433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.818595</td>\n",
       "      <td>3.637190</td>\n",
       "      <td>7.274379</td>\n",
       "      <td>0.454649</td>\n",
       "      <td>0.303099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.726419</td>\n",
       "      <td>3.452837</td>\n",
       "      <td>6.905675</td>\n",
       "      <td>0.431605</td>\n",
       "      <td>0.287736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.616993</td>\n",
       "      <td>3.233986</td>\n",
       "      <td>6.467971</td>\n",
       "      <td>0.404248</td>\n",
       "      <td>0.269499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.491410</td>\n",
       "      <td>2.982821</td>\n",
       "      <td>5.965642</td>\n",
       "      <td>0.372853</td>\n",
       "      <td>0.248568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.350926</td>\n",
       "      <td>2.701853</td>\n",
       "      <td>5.403705</td>\n",
       "      <td>0.337732</td>\n",
       "      <td>0.225154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.196944</td>\n",
       "      <td>2.393889</td>\n",
       "      <td>4.787777</td>\n",
       "      <td>0.299236</td>\n",
       "      <td>0.199491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.031003</td>\n",
       "      <td>2.062005</td>\n",
       "      <td>4.124011</td>\n",
       "      <td>0.257751</td>\n",
       "      <td>0.171834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.854760</td>\n",
       "      <td>1.709520</td>\n",
       "      <td>3.419039</td>\n",
       "      <td>0.213690</td>\n",
       "      <td>0.142460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.669976</td>\n",
       "      <td>1.339953</td>\n",
       "      <td>2.679905</td>\n",
       "      <td>0.167494</td>\n",
       "      <td>0.111663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.478499</td>\n",
       "      <td>0.956997</td>\n",
       "      <td>1.913995</td>\n",
       "      <td>0.119625</td>\n",
       "      <td>0.079750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>-1.795935</td>\n",
       "      <td>-3.591870</td>\n",
       "      <td>-7.183740</td>\n",
       "      <td>-0.448984</td>\n",
       "      <td>-0.299322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>-1.874829</td>\n",
       "      <td>-3.749657</td>\n",
       "      <td>-7.499314</td>\n",
       "      <td>-0.468707</td>\n",
       "      <td>-0.312471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>-1.934989</td>\n",
       "      <td>-3.869979</td>\n",
       "      <td>-7.739958</td>\n",
       "      <td>-0.483747</td>\n",
       "      <td>-0.322498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>-1.975817</td>\n",
       "      <td>-3.951633</td>\n",
       "      <td>-7.903266</td>\n",
       "      <td>-0.493954</td>\n",
       "      <td>-0.329303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>-1.996902</td>\n",
       "      <td>-3.993804</td>\n",
       "      <td>-7.987608</td>\n",
       "      <td>-0.499225</td>\n",
       "      <td>-0.332817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>-1.998035</td>\n",
       "      <td>-3.996070</td>\n",
       "      <td>-7.992140</td>\n",
       "      <td>-0.499509</td>\n",
       "      <td>-0.333006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>-1.979204</td>\n",
       "      <td>-3.958409</td>\n",
       "      <td>-7.916817</td>\n",
       "      <td>-0.494801</td>\n",
       "      <td>-0.329867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>-1.940598</td>\n",
       "      <td>-3.881196</td>\n",
       "      <td>-7.762392</td>\n",
       "      <td>-0.485150</td>\n",
       "      <td>-0.323433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>-1.882602</td>\n",
       "      <td>-3.765204</td>\n",
       "      <td>-7.530408</td>\n",
       "      <td>-0.470650</td>\n",
       "      <td>-0.313767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>-1.805796</td>\n",
       "      <td>-3.611591</td>\n",
       "      <td>-7.223182</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.300966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>-1.710946</td>\n",
       "      <td>-3.421893</td>\n",
       "      <td>-6.843785</td>\n",
       "      <td>-0.427737</td>\n",
       "      <td>-0.285158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>-1.599002</td>\n",
       "      <td>-3.198004</td>\n",
       "      <td>-6.396007</td>\n",
       "      <td>-0.399750</td>\n",
       "      <td>-0.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>-1.471081</td>\n",
       "      <td>-2.942161</td>\n",
       "      <td>-5.884322</td>\n",
       "      <td>-0.367770</td>\n",
       "      <td>-0.245180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>-1.328461</td>\n",
       "      <td>-2.656922</td>\n",
       "      <td>-5.313843</td>\n",
       "      <td>-0.332115</td>\n",
       "      <td>-0.221410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>-1.172568</td>\n",
       "      <td>-2.345135</td>\n",
       "      <td>-4.690270</td>\n",
       "      <td>-0.293142</td>\n",
       "      <td>-0.195428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>-1.004958</td>\n",
       "      <td>-2.009917</td>\n",
       "      <td>-4.019833</td>\n",
       "      <td>-0.251240</td>\n",
       "      <td>-0.167493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>-0.827308</td>\n",
       "      <td>-1.654616</td>\n",
       "      <td>-3.309232</td>\n",
       "      <td>-0.206827</td>\n",
       "      <td>-0.137885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>-0.641391</td>\n",
       "      <td>-1.282783</td>\n",
       "      <td>-2.565565</td>\n",
       "      <td>-0.160348</td>\n",
       "      <td>-0.106899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>-0.449066</td>\n",
       "      <td>-0.898132</td>\n",
       "      <td>-1.796265</td>\n",
       "      <td>-0.112267</td>\n",
       "      <td>-0.074844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>-0.252254</td>\n",
       "      <td>-0.504508</td>\n",
       "      <td>-1.009016</td>\n",
       "      <td>-0.063064</td>\n",
       "      <td>-0.042042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>-0.052922</td>\n",
       "      <td>-0.105843</td>\n",
       "      <td>-0.211686</td>\n",
       "      <td>-0.013230</td>\n",
       "      <td>-0.008820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>0.146940</td>\n",
       "      <td>0.293880</td>\n",
       "      <td>0.587759</td>\n",
       "      <td>0.036735</td>\n",
       "      <td>0.024490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>0.345333</td>\n",
       "      <td>0.690666</td>\n",
       "      <td>1.381332</td>\n",
       "      <td>0.086333</td>\n",
       "      <td>0.057555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>0.540276</td>\n",
       "      <td>1.080551</td>\n",
       "      <td>2.161102</td>\n",
       "      <td>0.135069</td>\n",
       "      <td>0.090046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0.729820</td>\n",
       "      <td>1.459640</td>\n",
       "      <td>2.919280</td>\n",
       "      <td>0.182455</td>\n",
       "      <td>0.121637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.912072</td>\n",
       "      <td>1.824145</td>\n",
       "      <td>3.648289</td>\n",
       "      <td>0.228018</td>\n",
       "      <td>0.152012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.085212</td>\n",
       "      <td>2.170423</td>\n",
       "      <td>4.340846</td>\n",
       "      <td>0.271303</td>\n",
       "      <td>0.180869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.247508</td>\n",
       "      <td>2.495015</td>\n",
       "      <td>4.990031</td>\n",
       "      <td>0.311877</td>\n",
       "      <td>0.207918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.397339</td>\n",
       "      <td>2.794678</td>\n",
       "      <td>5.589356</td>\n",
       "      <td>0.349335</td>\n",
       "      <td>0.232890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.533209</td>\n",
       "      <td>3.066417</td>\n",
       "      <td>6.132835</td>\n",
       "      <td>0.383302</td>\n",
       "      <td>0.255535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      amplitude_2  amplitude_4  amplitude_8  amplitude_02  amplitude_03\n",
       "0        0.000000     0.000000     0.000000      0.000000      0.000000\n",
       "1        0.199667     0.399334     0.798667      0.049917      0.033278\n",
       "2        0.397339     0.794677     1.589355      0.099335      0.066223\n",
       "3        0.591040     1.182081     2.364162      0.147760      0.098507\n",
       "4        0.778837     1.557673     3.115347      0.194709      0.129806\n",
       "5        0.958851     1.917702     3.835404      0.239713      0.159809\n",
       "6        1.129285     2.258570     4.517140      0.282321      0.188214\n",
       "7        1.288435     2.576871     5.153741      0.322109      0.214739\n",
       "8        1.434712     2.869424     5.738849      0.358678      0.239119\n",
       "9        1.566654     3.133308     6.266615      0.391663      0.261109\n",
       "10       1.682942     3.365884     6.731768      0.420735      0.280490\n",
       "11       1.782415     3.564829     7.129659      0.445604      0.297069\n",
       "12       1.864078     3.728156     7.456313      0.466020      0.310680\n",
       "13       1.927116     3.854233     7.708465      0.481779      0.321186\n",
       "14       1.970899     3.941799     7.883598      0.492725      0.328483\n",
       "15       1.994990     3.989980     7.979960      0.498747      0.332498\n",
       "16       1.999147     3.998294     7.996589      0.499787      0.333191\n",
       "17       1.983330     3.966659     7.933318      0.495832      0.330555\n",
       "18       1.947695     3.895391     7.790781      0.486924      0.324616\n",
       "19       1.892600     3.785200     7.570401      0.473150      0.315433\n",
       "20       1.818595     3.637190     7.274379      0.454649      0.303099\n",
       "21       1.726419     3.452837     6.905675      0.431605      0.287736\n",
       "22       1.616993     3.233986     6.467971      0.404248      0.269499\n",
       "23       1.491410     2.982821     5.965642      0.372853      0.248568\n",
       "24       1.350926     2.701853     5.403705      0.337732      0.225154\n",
       "25       1.196944     2.393889     4.787777      0.299236      0.199491\n",
       "26       1.031003     2.062005     4.124011      0.257751      0.171834\n",
       "27       0.854760     1.709520     3.419039      0.213690      0.142460\n",
       "28       0.669976     1.339953     2.679905      0.167494      0.111663\n",
       "29       0.478499     0.956997     1.913995      0.119625      0.079750\n",
       "...           ...          ...          ...           ...           ...\n",
       "9970    -1.795935    -3.591870    -7.183740     -0.448984     -0.299322\n",
       "9971    -1.874829    -3.749657    -7.499314     -0.468707     -0.312471\n",
       "9972    -1.934989    -3.869979    -7.739958     -0.483747     -0.322498\n",
       "9973    -1.975817    -3.951633    -7.903266     -0.493954     -0.329303\n",
       "9974    -1.996902    -3.993804    -7.987608     -0.499225     -0.332817\n",
       "9975    -1.998035    -3.996070    -7.992140     -0.499509     -0.333006\n",
       "9976    -1.979204    -3.958409    -7.916817     -0.494801     -0.329867\n",
       "9977    -1.940598    -3.881196    -7.762392     -0.485150     -0.323433\n",
       "9978    -1.882602    -3.765204    -7.530408     -0.470650     -0.313767\n",
       "9979    -1.805796    -3.611591    -7.223182     -0.451449     -0.300966\n",
       "9980    -1.710946    -3.421893    -6.843785     -0.427737     -0.285158\n",
       "9981    -1.599002    -3.198004    -6.396007     -0.399750     -0.266500\n",
       "9982    -1.471081    -2.942161    -5.884322     -0.367770     -0.245180\n",
       "9983    -1.328461    -2.656922    -5.313843     -0.332115     -0.221410\n",
       "9984    -1.172568    -2.345135    -4.690270     -0.293142     -0.195428\n",
       "9985    -1.004958    -2.009917    -4.019833     -0.251240     -0.167493\n",
       "9986    -0.827308    -1.654616    -3.309232     -0.206827     -0.137885\n",
       "9987    -0.641391    -1.282783    -2.565565     -0.160348     -0.106899\n",
       "9988    -0.449066    -0.898132    -1.796265     -0.112267     -0.074844\n",
       "9989    -0.252254    -0.504508    -1.009016     -0.063064     -0.042042\n",
       "9990    -0.052922    -0.105843    -0.211686     -0.013230     -0.008820\n",
       "9991     0.146940     0.293880     0.587759      0.036735      0.024490\n",
       "9992     0.345333     0.690666     1.381332      0.086333      0.057555\n",
       "9993     0.540276     1.080551     2.161102      0.135069      0.090046\n",
       "9994     0.729820     1.459640     2.919280      0.182455      0.121637\n",
       "9995     0.912072     1.824145     3.648289      0.228018      0.152012\n",
       "9996     1.085212     2.170423     4.340846      0.271303      0.180869\n",
       "9997     1.247508     2.495015     4.990031      0.311877      0.207918\n",
       "9998     1.397339     2.794678     5.589356      0.349335      0.232890\n",
       "9999     1.533209     3.066417     6.132835      0.383302      0.255535\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-09d1ca54b263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.tensor(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_tensor.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_tensor = torch_tensor.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper_Params\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-6\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train = TensorsDataset(torch_tensor.float())\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(train, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vals = dataiter.next()\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.LSTM(10000, 400, num_layers = 2)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(20, 20)\n",
    "        self.rnn3 = nn.LSTM(20, 400, num_layers = 1)\n",
    "        self.rnn4 = nn.LSTM(400, 10000, num_layers = 2)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.rnn1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.rnn3(z))\n",
    "        return F.sigmoid(self.rnn4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 1000\n",
    "input_size = 1\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "        \n",
    "        print(data.shape)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.view(-1,1,1)\n",
    "            \n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-e6eab45e804f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#test(epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-14406c70db0f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-2f190c894db0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-2f190c894db0>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc21\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc22\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    124\u001b[0m             raise RuntimeError(\n\u001b[0;32m    125\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m--> 126\u001b[1;33m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "for epoch in range(1000):\n",
    "    train(epoch)\n",
    "    #test(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get x values of the sine wave\n",
    "\n",
    "time        = np.arange(0, 1000, 0.1);\n",
    "\n",
    "# Amplitude of the sine wave is sine of a variable like time\n",
    "\n",
    "x1   = np.sin(time)\n",
    "#Create multiple amplitude arrays of varying size\n",
    "x2 = x1 * 2\n",
    "x3 = x1 *4\n",
    "y = x1 * 1/3\n",
    "\n",
    "\n",
    "\n",
    "eps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    def __init__(self, dset, bs=1):\n",
    "        self.dset = torch.LongTensor(dset).cuda()\n",
    "        self.len = len(self.dset)\n",
    "        self.idx = 0\n",
    "        self.bs = bs\n",
    "    def __len__(self):\n",
    "        return len(self.dset)\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        batch = [t for t in [torch.LongTensor(s.T).cuda() for s in np.stack([self.dset[i] for i in range(self.idx, self.idx + self.bs)]).T]] \n",
    "        self.idx = self.idx + self.bs\n",
    "        if self.idx > self.len - self.bs:\n",
    "            raise StopIteration\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = DataGenerator(np.stack([x1,x2,x3,y], axis=1), bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0], device='cuda:0'), tensor([-1, -1], device='cuda:0'), tensor([-3, -3], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "*Xs, yt = next(it)\n",
    "print(Xs)\n",
    "#(*[Variable(x) for x in Xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
