{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "# Import comet_ml in the top of your file\n",
    "from comet_ml import Experiment\n",
    "##Needs to be imported before sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn, optim, sigmoid\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import modules\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from torchaudio import transforms\n",
    "#from torchaudio import Datasets\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "#from torchviz import make_dot, make_dot_from_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check device type\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_creator(T = 20, L = 10000, N = 1000):\n",
    "    np.random.seed(2)\n",
    "\n",
    "\n",
    "\n",
    "    x = np.empty((N, L), 'int64')\n",
    "    x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "    data = np.sin(x / 1.0 / T).astype('float64')\n",
    "    data = sklearn.preprocessing.normalize(data)\n",
    "\n",
    "    torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input to LSTM is sequence length, batch, input_size   \n",
    "#sequence length ---- how many time steps RNN\n",
    "#batch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAE_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RAE_model, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.modules.GRU( 1000, 400, bias = True)\n",
    "        self.rnn2 = nn.modules.GRU( 400, 100, bias = True)\n",
    "        self.fc21 = nn.Linear(100, 20)\n",
    "        self.fc22 = nn.Linear(100, 20)\n",
    "        self.fc3 = nn.Linear(20, 100)\n",
    "        self.rnn2 = nn.modules.GRU(10, 100, 400, bias = True)\n",
    "        self.rnn2 = nn.modules.GRU(10, 400, 1000, bias = True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.rnn1((20,x)))\n",
    "        h2 = F.relu(self.rnn2(h1))\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.rnn3(h3))\n",
    "        return sigmoid(self.rnn4(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RVAE_model(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent VAE Model with GRUcells sequence length needs to be added\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(RVAE_model, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.modules.GRUCell(1000, 400, bias = True)\n",
    "        self.rnn2 = nn.modules.GRUCell(400, 100, bias = True)\n",
    "        self.fc21 = nn.Linear(100, 20)\n",
    "        self.fc22 = nn.Linear(100, 20)\n",
    "        self.fc3 = nn.Linear(20, 100)\n",
    "        self.rnn2 = nn.modules.GRUCell(100, 400, bias = True)\n",
    "        self.rnn2 = nn.modules.GRUCell(400, 1000, bias = True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.rnn1(x))\n",
    "        h2 = F.relu(self.rnn2(h1))\n",
    "        return self.fc21(h2), self.fc22(h2)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        h4 = F.relu(self.rnn3(h3))\n",
    "        return sigmoid(self.rnn4(h4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(1000, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 1000)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Developed by \n",
    "        \n",
    "        \"\"\"\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(1000, 400)\n",
    "        self.fc2 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 1000)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return F.relu(self.fc2(h1))\n",
    "        \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return F.relu(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        return self.decode(z)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sineDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = torch.load('traindata.pt')\n",
    "        self.label = []\n",
    "        vals = len(self.data)\n",
    "        for val in range(vals):\n",
    "            self.label.append(val)\n",
    "        if torch.cuda.is_available():\n",
    "            self.data, self.label = map(torch.cuda.FloatTensor, (self.data.T, self.label))\n",
    "        else:\n",
    "            self.data, self.label = map(torch.FloatTensor, (self.data.T, self.label))\n",
    "\n",
    "        \n",
    "        print(\"shape = \", self.data.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = RAE_model().to(device)\n",
    "    #model = RAE_model().to(device)\n",
    "\n",
    "    \n",
    "    return model, optim.Adam(model.parameters(), lr = lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_dataset, test_dataset, bs):\n",
    "    return (DataLoader(train_dataset, batch_size = batch_size),\n",
    "    DataLoader(test_dataset, batch_size = batch_size * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Need data_creator function\n",
    "\n",
    "data_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1000\n",
    "latent = 5\n",
    "input_dim = 5\n",
    "epochs = 1000\n",
    "seq_len = 32\n",
    "batch_size = 32\n",
    "input_size = 1000\n",
    "\n",
    "features = 997\n",
    "latent =50\n",
    "\n",
    "\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape =  torch.Size([10000, 1000])\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    " \n",
    "full_dataset = sineDataset( )\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "train_dl, test_dl = get_data(train_dataset, test_dataset, batch_size)\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_batch, mu, logvar, x):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    MSE = F.binary_cross_entropy(recon_batch, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def loss_function(output, x):\n",
    "    return F.binary_cross_entropy(output, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_function, xb, opt=None):\n",
    "    recon_batch, mu,logvar = model(xb)\n",
    "    loss = loss_function(recon_batch, mu,logvar, xb)\n",
    "   \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def loss_batch(model, loss_function, xb, opt=None):\n",
    "    output = model(xb)\n",
    "    loss = loss_function(output, xb)\n",
    "   \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        \n",
    "        for xb in train_dl:\n",
    "            \n",
    "            \n",
    "            #xb = xb.reshape(1,batch_size,input_size).to(device)  \n",
    "           \n",
    "            if torch.cuda.is_available():\n",
    "                xb = Variable(torch.cuda.FloatTensor(xb))\n",
    "            else:\n",
    "                xb = Variable(torch.FloatTensor(xb))\n",
    "            \n",
    "            \"\"\"seq_len, batch_size, input_size\n",
    "            input_size is 1000 seperate strings.\n",
    "            batch_size = 16\n",
    "            seq_len can be anything....\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            loss_batch(model, loss_func, xb, opt)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, loss_func, xb)\n",
    "                                    for xb in test_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full training in 3 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model, opt = get_model()\n",
    "#x = torch.randn(1, 10000)\n",
    "#make_dot(model(x), params=dict(model.named_parameters()))\n",
    "\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(epochs, model, loss_function, opt, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}   - need look this ---https://vxlabs.com/2017/12/08/variational-autoencoder-in-pytorch-commented-and-annotated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
