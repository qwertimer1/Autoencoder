{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "# Import comet_ml in the top of your file\n",
    "from comet_ml import Experiment\n",
    "##Needs to be imported before sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn, optim, sigmoid\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn import modules\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from torchaudio import transforms\n",
    "#from torchaudio import Datasets\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "from glob import glob\n",
    "import datetime\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "#from torchviz import make_dot, make_dot_from_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check device type\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_creator(T = 20, L = 10000, N = 1000):\n",
    "    np.random.seed(2)\n",
    "\n",
    "\n",
    "\n",
    "    x = np.empty((N, L), 'int64')\n",
    "    x[:] = np.array(range(L)) + np.random.randint(-4 * T, 4 * T, N).reshape(N, 1)\n",
    "    data = np.sin(x / 1.0 / T).astype('float64')\n",
    "    data = sklearn.preprocessing.normalize(data)\n",
    "\n",
    "    torch.save(data, open('traindata.pt', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input to LSTM is sequence length, batch, input_size   \n",
    "#sequence length ---- how many time steps RNN\n",
    "#batch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AE_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE_model, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(10000, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 10000)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RVAE(nn.Module):\n",
    "    nn.Sequential(\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sineDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = torch.load('traindata.pt')\n",
    "        self.label = []\n",
    "        vals = len(self.data)\n",
    "        for val in range(vals):\n",
    "            self.label.append(val)\n",
    "\n",
    "        self.data, self.label = map(torch.FloatTensor, (self.data, self.label))\n",
    "        \n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = AE_model().to(device)\n",
    "\n",
    "    \n",
    "    return model, optim.Adam(model.parameters(), lr = lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(train_dataset, test_dataset, bs):\n",
    "    return (DataLoader(train_dataset, batch_size = batch_size),\n",
    "    DataLoader(test_dataset, batch_size = batch_size * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Need data_creator function\n",
    "\n",
    "data_creator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim = 10000\n",
    "latent = 5\n",
    "input_dim = 5\n",
    "epochs = 10000\n",
    "seq_len = 10\n",
    "batch_size = 32\n",
    "input_size = 997\n",
    "\n",
    "features = 997\n",
    "latent =50\n",
    "\n",
    "\n",
    "lr = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "full_dataset = sineDataset( )\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "train_dl, test_dl = get_data(train_dataset, test_dataset, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_batch, mu, logvar, x):\n",
    "    \"\"\"\n",
    "    recon_x: generating images\n",
    "    x: origin images\n",
    "    mu: latent mean\n",
    "    logvar: latent log variance\n",
    "    \"\"\"\n",
    "    MSE = F.binary_cross_entropy(recon_batch, x)  # mse loss\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_function, xb, opt=None):\n",
    "    recon_batch, mu,logvar = model(xb)\n",
    "    loss = loss_function(recon_batch, mu,logvar, xb)\n",
    "   \n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for xb in train_dl: \n",
    "            \n",
    "            loss_batch(model, loss_func, xb, opt)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                losses,nums = zip(*[loss_batch(model, loss_func, xb)\n",
    "                                    for xb in test_dl])\n",
    "            val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "            print(epoch, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full training in 3 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model, opt = get_model()\n",
    "#x = torch.randn(1, 10000)\n",
    "#make_dot(model(x), params=dict(model.named_parameters()))\n",
    "\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.4663240838050842\n",
      "0 1.464477379322052\n",
      "0 1.463164553642273\n",
      "0 1.4615251898765564\n",
      "0 1.4604030585289\n",
      "0 1.458550145626068\n",
      "0 1.4568650484085084\n",
      "0 1.4558627533912658\n",
      "0 1.454238338470459\n",
      "0 1.452788963317871\n",
      "0 1.4512803220748902\n",
      "0 1.449676902294159\n",
      "0 1.4485812425613402\n",
      "0 1.4470227694511413\n",
      "0 1.4456518840789796\n",
      "0 1.444277195930481\n",
      "0 1.442923254966736\n",
      "0 1.4415382027626038\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-496faadfb159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-a7580647ea2d>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(epochs, model, loss_func, opt, train_dl, test_dl)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 losses,nums = zip(*[loss_batch(model, loss_func, xb)\n\u001b[1;32m---> 11\u001b[1;33m                                     for xb in test_dl])\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-a7580647ea2d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 losses,nums = zip(*[loss_batch(model, loss_func, xb)\n\u001b[1;32m---> 11\u001b[1;33m                                     for xb in test_dl])\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-79fce880143c>\u001b[0m in \u001b[0;36mloss_batch\u001b[1;34m(model, loss_function, xb, opt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlogvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-7a53372fd009>\u001b[0m in \u001b[0;36mloss_function\u001b[1;34m(recon_batch, mu, logvar, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlogvar\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0mlog\u001b[0m \u001b[0mvariance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# mse loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;31m# loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mKLD_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   1601\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1603\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fit(epochs, model, loss_function, opt, train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Page Break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent):\n",
    "        super(AE, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent = latent\n",
    "        self.gru = nn.GRU(input_size, self.hidden_size)\n",
    "        self.linear = nn.Linear(input_size, self.latent)\n",
    "        self.gru2 = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        output, hidden = self.gru(input, self.hidden_size)\n",
    "        fc1 = self.linear(output)\n",
    "        output2, hidden = self.gru(fc1, self.hidden_size)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        \n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1,1, self.hidden_size, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, size_average=False)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    #KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE #+ KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page Break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get x values of the sine wave\n",
    "\n",
    "time        = np.arange(0, 1000, 0.1);\n",
    "\n",
    "# Amplitude of the sine wave is sine of a variable like time\n",
    "\n",
    "amplitude   = np.sin(time)\n",
    "#Create multiple amplitude arrays of varying size\n",
    "amplitude_2 = amplitude * 2\n",
    "amplitude_4 = amplitude *4\n",
    "amplitude_03 = amplitude * 1/3\n",
    "amplitude_02 = amplitude * 1/2\n",
    "amplitude_8 = amplitude * 8\n",
    "amplitudes = {'amplitude_2':amplitude_2, \n",
    "              'amplitude_4':amplitude_4, \n",
    "              'amplitude_8':amplitude_8, \n",
    "              'amplitude_02':amplitude_02,\n",
    "              'amplitude_03':amplitude_03}\n",
    "\n",
    "amplitudes_nd = [amplitude_2, \n",
    "              amplitude_4, \n",
    "              amplitude_8, \n",
    "              amplitude_02,\n",
    "              amplitude_03]\n",
    "\n",
    "\n",
    "eps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(amplitudes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amplitude_2</th>\n",
       "      <th>amplitude_4</th>\n",
       "      <th>amplitude_8</th>\n",
       "      <th>amplitude_02</th>\n",
       "      <th>amplitude_03</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.199667</td>\n",
       "      <td>0.399334</td>\n",
       "      <td>0.798667</td>\n",
       "      <td>0.049917</td>\n",
       "      <td>0.033278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.397339</td>\n",
       "      <td>0.794677</td>\n",
       "      <td>1.589355</td>\n",
       "      <td>0.099335</td>\n",
       "      <td>0.066223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.591040</td>\n",
       "      <td>1.182081</td>\n",
       "      <td>2.364162</td>\n",
       "      <td>0.147760</td>\n",
       "      <td>0.098507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.778837</td>\n",
       "      <td>1.557673</td>\n",
       "      <td>3.115347</td>\n",
       "      <td>0.194709</td>\n",
       "      <td>0.129806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.958851</td>\n",
       "      <td>1.917702</td>\n",
       "      <td>3.835404</td>\n",
       "      <td>0.239713</td>\n",
       "      <td>0.159809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.129285</td>\n",
       "      <td>2.258570</td>\n",
       "      <td>4.517140</td>\n",
       "      <td>0.282321</td>\n",
       "      <td>0.188214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.288435</td>\n",
       "      <td>2.576871</td>\n",
       "      <td>5.153741</td>\n",
       "      <td>0.322109</td>\n",
       "      <td>0.214739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.434712</td>\n",
       "      <td>2.869424</td>\n",
       "      <td>5.738849</td>\n",
       "      <td>0.358678</td>\n",
       "      <td>0.239119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.566654</td>\n",
       "      <td>3.133308</td>\n",
       "      <td>6.266615</td>\n",
       "      <td>0.391663</td>\n",
       "      <td>0.261109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.682942</td>\n",
       "      <td>3.365884</td>\n",
       "      <td>6.731768</td>\n",
       "      <td>0.420735</td>\n",
       "      <td>0.280490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.782415</td>\n",
       "      <td>3.564829</td>\n",
       "      <td>7.129659</td>\n",
       "      <td>0.445604</td>\n",
       "      <td>0.297069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.864078</td>\n",
       "      <td>3.728156</td>\n",
       "      <td>7.456313</td>\n",
       "      <td>0.466020</td>\n",
       "      <td>0.310680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.927116</td>\n",
       "      <td>3.854233</td>\n",
       "      <td>7.708465</td>\n",
       "      <td>0.481779</td>\n",
       "      <td>0.321186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.970899</td>\n",
       "      <td>3.941799</td>\n",
       "      <td>7.883598</td>\n",
       "      <td>0.492725</td>\n",
       "      <td>0.328483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.994990</td>\n",
       "      <td>3.989980</td>\n",
       "      <td>7.979960</td>\n",
       "      <td>0.498747</td>\n",
       "      <td>0.332498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.999147</td>\n",
       "      <td>3.998294</td>\n",
       "      <td>7.996589</td>\n",
       "      <td>0.499787</td>\n",
       "      <td>0.333191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.983330</td>\n",
       "      <td>3.966659</td>\n",
       "      <td>7.933318</td>\n",
       "      <td>0.495832</td>\n",
       "      <td>0.330555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.947695</td>\n",
       "      <td>3.895391</td>\n",
       "      <td>7.790781</td>\n",
       "      <td>0.486924</td>\n",
       "      <td>0.324616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.892600</td>\n",
       "      <td>3.785200</td>\n",
       "      <td>7.570401</td>\n",
       "      <td>0.473150</td>\n",
       "      <td>0.315433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.818595</td>\n",
       "      <td>3.637190</td>\n",
       "      <td>7.274379</td>\n",
       "      <td>0.454649</td>\n",
       "      <td>0.303099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.726419</td>\n",
       "      <td>3.452837</td>\n",
       "      <td>6.905675</td>\n",
       "      <td>0.431605</td>\n",
       "      <td>0.287736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.616993</td>\n",
       "      <td>3.233986</td>\n",
       "      <td>6.467971</td>\n",
       "      <td>0.404248</td>\n",
       "      <td>0.269499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.491410</td>\n",
       "      <td>2.982821</td>\n",
       "      <td>5.965642</td>\n",
       "      <td>0.372853</td>\n",
       "      <td>0.248568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.350926</td>\n",
       "      <td>2.701853</td>\n",
       "      <td>5.403705</td>\n",
       "      <td>0.337732</td>\n",
       "      <td>0.225154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.196944</td>\n",
       "      <td>2.393889</td>\n",
       "      <td>4.787777</td>\n",
       "      <td>0.299236</td>\n",
       "      <td>0.199491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.031003</td>\n",
       "      <td>2.062005</td>\n",
       "      <td>4.124011</td>\n",
       "      <td>0.257751</td>\n",
       "      <td>0.171834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.854760</td>\n",
       "      <td>1.709520</td>\n",
       "      <td>3.419039</td>\n",
       "      <td>0.213690</td>\n",
       "      <td>0.142460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.669976</td>\n",
       "      <td>1.339953</td>\n",
       "      <td>2.679905</td>\n",
       "      <td>0.167494</td>\n",
       "      <td>0.111663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.478499</td>\n",
       "      <td>0.956997</td>\n",
       "      <td>1.913995</td>\n",
       "      <td>0.119625</td>\n",
       "      <td>0.079750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>-1.795935</td>\n",
       "      <td>-3.591870</td>\n",
       "      <td>-7.183740</td>\n",
       "      <td>-0.448984</td>\n",
       "      <td>-0.299322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>-1.874829</td>\n",
       "      <td>-3.749657</td>\n",
       "      <td>-7.499314</td>\n",
       "      <td>-0.468707</td>\n",
       "      <td>-0.312471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>-1.934989</td>\n",
       "      <td>-3.869979</td>\n",
       "      <td>-7.739958</td>\n",
       "      <td>-0.483747</td>\n",
       "      <td>-0.322498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>-1.975817</td>\n",
       "      <td>-3.951633</td>\n",
       "      <td>-7.903266</td>\n",
       "      <td>-0.493954</td>\n",
       "      <td>-0.329303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>-1.996902</td>\n",
       "      <td>-3.993804</td>\n",
       "      <td>-7.987608</td>\n",
       "      <td>-0.499225</td>\n",
       "      <td>-0.332817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>-1.998035</td>\n",
       "      <td>-3.996070</td>\n",
       "      <td>-7.992140</td>\n",
       "      <td>-0.499509</td>\n",
       "      <td>-0.333006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>-1.979204</td>\n",
       "      <td>-3.958409</td>\n",
       "      <td>-7.916817</td>\n",
       "      <td>-0.494801</td>\n",
       "      <td>-0.329867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>-1.940598</td>\n",
       "      <td>-3.881196</td>\n",
       "      <td>-7.762392</td>\n",
       "      <td>-0.485150</td>\n",
       "      <td>-0.323433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>-1.882602</td>\n",
       "      <td>-3.765204</td>\n",
       "      <td>-7.530408</td>\n",
       "      <td>-0.470650</td>\n",
       "      <td>-0.313767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>-1.805796</td>\n",
       "      <td>-3.611591</td>\n",
       "      <td>-7.223182</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.300966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>-1.710946</td>\n",
       "      <td>-3.421893</td>\n",
       "      <td>-6.843785</td>\n",
       "      <td>-0.427737</td>\n",
       "      <td>-0.285158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>-1.599002</td>\n",
       "      <td>-3.198004</td>\n",
       "      <td>-6.396007</td>\n",
       "      <td>-0.399750</td>\n",
       "      <td>-0.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>-1.471081</td>\n",
       "      <td>-2.942161</td>\n",
       "      <td>-5.884322</td>\n",
       "      <td>-0.367770</td>\n",
       "      <td>-0.245180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>-1.328461</td>\n",
       "      <td>-2.656922</td>\n",
       "      <td>-5.313843</td>\n",
       "      <td>-0.332115</td>\n",
       "      <td>-0.221410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>-1.172568</td>\n",
       "      <td>-2.345135</td>\n",
       "      <td>-4.690270</td>\n",
       "      <td>-0.293142</td>\n",
       "      <td>-0.195428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>-1.004958</td>\n",
       "      <td>-2.009917</td>\n",
       "      <td>-4.019833</td>\n",
       "      <td>-0.251240</td>\n",
       "      <td>-0.167493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>-0.827308</td>\n",
       "      <td>-1.654616</td>\n",
       "      <td>-3.309232</td>\n",
       "      <td>-0.206827</td>\n",
       "      <td>-0.137885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>-0.641391</td>\n",
       "      <td>-1.282783</td>\n",
       "      <td>-2.565565</td>\n",
       "      <td>-0.160348</td>\n",
       "      <td>-0.106899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>-0.449066</td>\n",
       "      <td>-0.898132</td>\n",
       "      <td>-1.796265</td>\n",
       "      <td>-0.112267</td>\n",
       "      <td>-0.074844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>-0.252254</td>\n",
       "      <td>-0.504508</td>\n",
       "      <td>-1.009016</td>\n",
       "      <td>-0.063064</td>\n",
       "      <td>-0.042042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>-0.052922</td>\n",
       "      <td>-0.105843</td>\n",
       "      <td>-0.211686</td>\n",
       "      <td>-0.013230</td>\n",
       "      <td>-0.008820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>0.146940</td>\n",
       "      <td>0.293880</td>\n",
       "      <td>0.587759</td>\n",
       "      <td>0.036735</td>\n",
       "      <td>0.024490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>0.345333</td>\n",
       "      <td>0.690666</td>\n",
       "      <td>1.381332</td>\n",
       "      <td>0.086333</td>\n",
       "      <td>0.057555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>0.540276</td>\n",
       "      <td>1.080551</td>\n",
       "      <td>2.161102</td>\n",
       "      <td>0.135069</td>\n",
       "      <td>0.090046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>0.729820</td>\n",
       "      <td>1.459640</td>\n",
       "      <td>2.919280</td>\n",
       "      <td>0.182455</td>\n",
       "      <td>0.121637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>0.912072</td>\n",
       "      <td>1.824145</td>\n",
       "      <td>3.648289</td>\n",
       "      <td>0.228018</td>\n",
       "      <td>0.152012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>1.085212</td>\n",
       "      <td>2.170423</td>\n",
       "      <td>4.340846</td>\n",
       "      <td>0.271303</td>\n",
       "      <td>0.180869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>1.247508</td>\n",
       "      <td>2.495015</td>\n",
       "      <td>4.990031</td>\n",
       "      <td>0.311877</td>\n",
       "      <td>0.207918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>1.397339</td>\n",
       "      <td>2.794678</td>\n",
       "      <td>5.589356</td>\n",
       "      <td>0.349335</td>\n",
       "      <td>0.232890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>1.533209</td>\n",
       "      <td>3.066417</td>\n",
       "      <td>6.132835</td>\n",
       "      <td>0.383302</td>\n",
       "      <td>0.255535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      amplitude_2  amplitude_4  amplitude_8  amplitude_02  amplitude_03\n",
       "0        0.000000     0.000000     0.000000      0.000000      0.000000\n",
       "1        0.199667     0.399334     0.798667      0.049917      0.033278\n",
       "2        0.397339     0.794677     1.589355      0.099335      0.066223\n",
       "3        0.591040     1.182081     2.364162      0.147760      0.098507\n",
       "4        0.778837     1.557673     3.115347      0.194709      0.129806\n",
       "5        0.958851     1.917702     3.835404      0.239713      0.159809\n",
       "6        1.129285     2.258570     4.517140      0.282321      0.188214\n",
       "7        1.288435     2.576871     5.153741      0.322109      0.214739\n",
       "8        1.434712     2.869424     5.738849      0.358678      0.239119\n",
       "9        1.566654     3.133308     6.266615      0.391663      0.261109\n",
       "10       1.682942     3.365884     6.731768      0.420735      0.280490\n",
       "11       1.782415     3.564829     7.129659      0.445604      0.297069\n",
       "12       1.864078     3.728156     7.456313      0.466020      0.310680\n",
       "13       1.927116     3.854233     7.708465      0.481779      0.321186\n",
       "14       1.970899     3.941799     7.883598      0.492725      0.328483\n",
       "15       1.994990     3.989980     7.979960      0.498747      0.332498\n",
       "16       1.999147     3.998294     7.996589      0.499787      0.333191\n",
       "17       1.983330     3.966659     7.933318      0.495832      0.330555\n",
       "18       1.947695     3.895391     7.790781      0.486924      0.324616\n",
       "19       1.892600     3.785200     7.570401      0.473150      0.315433\n",
       "20       1.818595     3.637190     7.274379      0.454649      0.303099\n",
       "21       1.726419     3.452837     6.905675      0.431605      0.287736\n",
       "22       1.616993     3.233986     6.467971      0.404248      0.269499\n",
       "23       1.491410     2.982821     5.965642      0.372853      0.248568\n",
       "24       1.350926     2.701853     5.403705      0.337732      0.225154\n",
       "25       1.196944     2.393889     4.787777      0.299236      0.199491\n",
       "26       1.031003     2.062005     4.124011      0.257751      0.171834\n",
       "27       0.854760     1.709520     3.419039      0.213690      0.142460\n",
       "28       0.669976     1.339953     2.679905      0.167494      0.111663\n",
       "29       0.478499     0.956997     1.913995      0.119625      0.079750\n",
       "...           ...          ...          ...           ...           ...\n",
       "9970    -1.795935    -3.591870    -7.183740     -0.448984     -0.299322\n",
       "9971    -1.874829    -3.749657    -7.499314     -0.468707     -0.312471\n",
       "9972    -1.934989    -3.869979    -7.739958     -0.483747     -0.322498\n",
       "9973    -1.975817    -3.951633    -7.903266     -0.493954     -0.329303\n",
       "9974    -1.996902    -3.993804    -7.987608     -0.499225     -0.332817\n",
       "9975    -1.998035    -3.996070    -7.992140     -0.499509     -0.333006\n",
       "9976    -1.979204    -3.958409    -7.916817     -0.494801     -0.329867\n",
       "9977    -1.940598    -3.881196    -7.762392     -0.485150     -0.323433\n",
       "9978    -1.882602    -3.765204    -7.530408     -0.470650     -0.313767\n",
       "9979    -1.805796    -3.611591    -7.223182     -0.451449     -0.300966\n",
       "9980    -1.710946    -3.421893    -6.843785     -0.427737     -0.285158\n",
       "9981    -1.599002    -3.198004    -6.396007     -0.399750     -0.266500\n",
       "9982    -1.471081    -2.942161    -5.884322     -0.367770     -0.245180\n",
       "9983    -1.328461    -2.656922    -5.313843     -0.332115     -0.221410\n",
       "9984    -1.172568    -2.345135    -4.690270     -0.293142     -0.195428\n",
       "9985    -1.004958    -2.009917    -4.019833     -0.251240     -0.167493\n",
       "9986    -0.827308    -1.654616    -3.309232     -0.206827     -0.137885\n",
       "9987    -0.641391    -1.282783    -2.565565     -0.160348     -0.106899\n",
       "9988    -0.449066    -0.898132    -1.796265     -0.112267     -0.074844\n",
       "9989    -0.252254    -0.504508    -1.009016     -0.063064     -0.042042\n",
       "9990    -0.052922    -0.105843    -0.211686     -0.013230     -0.008820\n",
       "9991     0.146940     0.293880     0.587759      0.036735      0.024490\n",
       "9992     0.345333     0.690666     1.381332      0.086333      0.057555\n",
       "9993     0.540276     1.080551     2.161102      0.135069      0.090046\n",
       "9994     0.729820     1.459640     2.919280      0.182455      0.121637\n",
       "9995     0.912072     1.824145     3.648289      0.228018      0.152012\n",
       "9996     1.085212     2.170423     4.340846      0.271303      0.180869\n",
       "9997     1.247508     2.495015     4.990031      0.311877      0.207918\n",
       "9998     1.397339     2.794678     5.589356      0.349335      0.232890\n",
       "9999     1.533209     3.066417     6.132835      0.383302      0.255535\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-09d1ca54b263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.tensor(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_tensor.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch_tensor = torch_tensor.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hyper_Params\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-6\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train = TensorsDataset(torch_tensor.float())\n",
    "\n",
    "train_loader= torch.utils.data.DataLoader(train, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "vals = dataiter.next()\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.rnn1 = nn.LSTM(10000, 400, num_layers = 2)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(20, 20)\n",
    "        self.rnn3 = nn.LSTM(20, 400, num_layers = 1)\n",
    "        self.rnn4 = nn.LSTM(400, 10000, num_layers = 2)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.rnn1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.rnn3(z))\n",
    "        return F.sigmoid(self.rnn4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 1000\n",
    "input_size = 1\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "\n",
    "        \n",
    "        print(data.shape)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            data = data.view(-1,1,1)\n",
    "            \n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input must have 3 dimensions, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-e6eab45e804f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#test(epoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-134-14406c70db0f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-2f190c894db0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-132-2f190c894db0>\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mh1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc21\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc22\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[0mflat_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         func = self._backend.RNN(\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    124\u001b[0m             raise RuntimeError(\n\u001b[0;32m    125\u001b[0m                 'input must have {} dimensions, got {}'.format(\n\u001b[1;32m--> 126\u001b[1;33m                     expected_input_dim, input.dim()))\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             raise RuntimeError(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input must have 3 dimensions, got 2"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "for epoch in range(1000):\n",
    "    train(epoch)\n",
    "    #test(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get x values of the sine wave\n",
    "\n",
    "time        = np.arange(0, 1000, 0.1);\n",
    "\n",
    "# Amplitude of the sine wave is sine of a variable like time\n",
    "\n",
    "x1   = np.sin(time)\n",
    "#Create multiple amplitude arrays of varying size\n",
    "x2 = x1 * 2\n",
    "x3 = x1 *4\n",
    "y = x1 * 1/3\n",
    "\n",
    "\n",
    "\n",
    "eps = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataGenerator():\n",
    "    def __init__(self, dset, bs=1):\n",
    "        self.dset = torch.LongTensor(dset).cuda()\n",
    "        self.len = len(self.dset)\n",
    "        self.idx = 0\n",
    "        self.bs = bs\n",
    "    def __len__(self):\n",
    "        return len(self.dset)\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        batch = [t for t in [torch.LongTensor(s.T).cuda() for s in np.stack([self.dset[i] for i in range(self.idx, self.idx + self.bs)]).T]] \n",
    "        self.idx = self.idx + self.bs\n",
    "        if self.idx > self.len - self.bs:\n",
    "            raise StopIteration\n",
    "        return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = DataGenerator(np.stack([x1,x2,x3,y], axis=1), bs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 0], device='cuda:0'), tensor([-1, -1], device='cuda:0'), tensor([-3, -3], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "*Xs, yt = next(it)\n",
    "print(Xs)\n",
    "#(*[Variable(x) for x in Xs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
